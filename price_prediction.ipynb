{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Assessment - Stock Price Prediction\n",
    "### Table of contents\n",
    "- Import libraries\n",
    "- Read in data\n",
    "- Data Preprocessing\n",
    "- Exploratory analysis\n",
    "- Preprocessing/Splitting function\n",
    "- Ridge Regression\n",
    "- Linear Regression\n",
    "- Exploratory Analysis Continued\n",
    "- Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data\n",
    "\n",
    "We have three data sets: `indexData`, `indexInfo`, and `indexProcessed`. The data and processed datasets have one difference: an extra column in the processed dataset that represents the local currency translated into USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"../data/archive/indexData.csv\")\n",
    "info_df = pd.read_csv(\"../data/archive/indexInfo.csv\")\n",
    "processed_df = pd.read_csv(\"../data/archive/indexProcessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    "From our three files, the important file is `indexProcessed` as that contains the most amount of relevant information to a regression. I also join the processed and info dataframes to collect all information at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(processed_df.columns.values) - set(data_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_info_df = pd.merge(processed_df, info_df, on='Index')\n",
    "processed_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_info_df['Date'] = pd.to_datetime(processed_info_df['Date'])\n",
    "processed_info_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis\n",
    "I create a heatmap and a pairplot of each of the features in `indexProcessed`. This shows that some fields are very highly correlated and some are not correlated whatsoever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(processed_info_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(processed_info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then claculate the index with the highest return over the period recorded in the data set. The index with the highest return is by far `IXIC`, with 14,000% return from start to finish. If you invested £1,000 in 1971, that money would now be worth £140,000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stocks = np.unique(processed_info_df['Index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=processed_info_df, x='Date', y='CloseUSD', hue='Index', lw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dfs = {}\n",
    "for stock in all_stocks:\n",
    "    data = processed_info_df[processed_info_df['Index'] == stock]\n",
    "    data = data[['Date', 'CloseUSD']]\n",
    "    data = data.set_index('Date')\n",
    "    stock_dfs[stock] = data\n",
    "stock_dfs['IXIC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_returns = {}\n",
    "\n",
    "for name, df in stock_dfs.items():\n",
    "    start_price = df[df.index == df.index.min()].iloc[0]['CloseUSD']\n",
    "    final_price = df[df.index == df.index.max()].iloc[0]['CloseUSD']\n",
    "    ret = (final_price / start_price) * 100\n",
    "    stock_returns[name] = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_returns_df = pd.DataFrame.from_dict(stock_returns, orient='index', columns=['Return %'])\n",
    "\n",
    "ax = sns.barplot(data=stock_returns_df,x=stock_returns_df.index,y='Return %')\n",
    "ax.set(xlabel='Index', ylabel='Return %')\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing/Splitting Functions\n",
    "These functions are the base for: \n",
    " - Picking the feature columns;\n",
    " - Scaling the data;\n",
    " - Splitting data into train-test splits;\n",
    " - Splitting data into dependent-indepdent splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_feature_columns(df):\n",
    "    feature_columns = ['CloseUSD']\n",
    "    df = df[feature_columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def min_max_scale_data(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_close_usd = scaler.fit_transform(df[['CloseUSD']])\n",
    "    df[['CloseUSD']] = scaled_close_usd\n",
    "    return scaler, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standard_scale_data(df):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_close_usd = scaler.fit_transform(df[['CloseUSD']])\n",
    "    df[['CloseUSD']] = scaled_close_usd\n",
    "    return scaler, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, training_split=0.8):\n",
    "    training_days = (df.index.max() - df.index.min())*training_split\n",
    "    cutoff_date = df.index.min() + training_days\n",
    "    test_data = df[df.index >= cutoff_date]\n",
    "    train_data = df[df.index < cutoff_date]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependent_independent_split(df, n_data_points=1):\n",
    "    x, y = [], []\n",
    "    for index in range(n_data_points, len(df)):\n",
    "        x.append(np.array(df[index-n_data_points:index]))\n",
    "        y.append(np.array(df[index:index+1]))        \n",
    "    x = np.reshape(np.array(x), (len(x), n_data_points))\n",
    "    y = np.reshape(np.array(y), (len(y), 1))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Data Processing/Training Functions\n",
    "These functions are responsible for using the functions above to handle a batch of preprocessing/training at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_DATA_POINTS = 90\n",
    "\n",
    "def process_df(df):\n",
    "    df = pick_feature_columns(df)\n",
    "    scaler, df = standard_scale_data(df)\n",
    "    train_data, test_data = train_test_split(df)\n",
    "    X_train, y_train = dependent_independent_split(train_data, NUMBER_DATA_POINTS)\n",
    "    X_test, y_test = dependent_independent_split(test_data, NUMBER_DATA_POINTS)\n",
    "    return X_train, X_test, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    mlr_model = LinearRegression(n_jobs=1)\n",
    "    mlr_model.fit(X_train, y_train)\n",
    "    return mlr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "def score_model(scaler, actual, predicted):\n",
    "    if scaler is not None:    \n",
    "        actual = scaler.inverse_transform(actual)\n",
    "        predicted = scaler.inverse_transform(predicted)\n",
    "    return {\n",
    "        \"RMSE\": mean_squared_error(actual, predicted, squared=False),\n",
    "        \"R2\": r2_score(actual, predicted),\n",
    "        \"MAPE\": mean_absolute_percentage_error(actual, predicted),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "For this first regression, we are picking the index `IXIC` as it has the highest return in the data set. I then use `GridSearchCV` to find the most optimal values for alpha and tol for the training data split. We then score the accuracy and plot the predicted values vs. the true values. We then re-create the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_DATA_POINTS = 120\n",
    "\n",
    "stock_data = stock_dfs['IXIC']\n",
    "stock_data = pick_feature_columns(stock_data)\n",
    "\n",
    "train_data, test_data = train_test_split(stock_data)\n",
    "X_train, y_train = dependent_independent_split(train_data, NUMBER_DATA_POINTS)\n",
    "X_test, y_test = dependent_independent_split(test_data, NUMBER_DATA_POINTS)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "plt.plot(train_data, 'b')\n",
    "plt.plot(test_data, 'r')\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"CloseUSD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lasso_model = Ridge()\n",
    "paramters = {\n",
    "    'alpha': [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8],\n",
    "    'tol': [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "}\n",
    "\n",
    "optimised_model = GridSearchCV(lasso_model, paramters)\n",
    "optimised_model.fit(X_train, y_train)\n",
    "\n",
    "print(optimised_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = optimised_model.predict(X_test)\n",
    "predictions = predictions.reshape(len(predictions), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print(\"RMSE: \",mean_squared_error(y_test, predictions, squared=False))\n",
    "print(\"R-squared: \", r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "plt.plot(predictions, 'r')\n",
    "plt.plot(y_test, 'b')\n",
    "plt.grid(True)\n",
    "ax.set_xlabel('Timestep')\n",
    "ax.set_ylabel('CloseUSD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mean = X_test.mean(axis=1)\n",
    "y_pred = optimised_model.predict(X_test)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "plt.scatter(X_test_mean, y_test, color='k')\n",
    "plt.plot(X_test_mean, y_pred, color='r')\n",
    "ax.set_xlabel('CloseUSD')\n",
    "ax.set_ylabel('CloseUSD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression on All Stocks\n",
    "This is running a ridge regression on all of the stocks in the data set, and finding the mean of all of the metrics that we have selected for our comparisons. We redefine `process_df` and `train_model` to include no scaling, and Ridge regression model respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_DATA_POINTS = 90\n",
    "\n",
    "def process_df(df):\n",
    "    df = pick_feature_columns(df)\n",
    "    train_data, test_data = train_test_split(df)\n",
    "    X_train, y_train = dependent_independent_split(train_data, NUMBER_DATA_POINTS)\n",
    "    X_test, y_test = dependent_independent_split(test_data, NUMBER_DATA_POINTS)\n",
    "    return X_train, X_test, y_train, y_test, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    model = Ridge(alpha=0.01, tol=0.1)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances = {}\n",
    "for name, df in stock_dfs.items():\n",
    "    X_train, X_test, y_train, y_test, scaler = process_df(df)\n",
    "    model = train_model(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    scores = score_model(scaler, y_test, predictions)\n",
    "    performances[name] = scores\n",
    "\n",
    "performances_df = pd.DataFrame.from_dict(data=performances, orient='index')\n",
    "performances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "For the first linear regression, I am repeating the same steps that I have previously done with Ridge regression, but with only one independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_DATA_POINTS = 1\n",
    "\n",
    "stock_data = stock_dfs['IXIC']\n",
    "stock_data = pick_feature_columns(stock_data)\n",
    "\n",
    "train_data, test_data = train_test_split(stock_data)\n",
    "X_train, y_train = dependent_independent_split(train_data, NUMBER_DATA_POINTS)\n",
    "X_test, y_test = dependent_independent_split(test_data, NUMBER_DATA_POINTS)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "plt.plot(train_data, 'b')\n",
    "plt.plot(test_data, 'r')\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"CloseUSD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression(n_jobs=1)\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print(\"RMSE: \", mean_squared_error(y_test, predictions, squared=False))\n",
    "print(\"R-squared: \", r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "plt.plot(predictions, 'r')\n",
    "plt.plot(y_test, 'b')\n",
    "plt.grid(True)\n",
    "ax.set_xlabel('Timestep')\n",
    "ax.set_ylabel('CloseUSD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph below clearly shows a linear regression, as it creates a straight line (red line) to predict the data found in the test set (black dots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mean = X_test.mean(axis=1)\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "plt.scatter(X_test_mean, y_test, color='k')\n",
    "plt.plot(X_test_mean, y_pred, color='r')\n",
    "ax.set_xlabel('CloseUSD')\n",
    "ax.set_ylabel('CloseUSD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Linear Regression\n",
    "This attempt is similar to the others, but with 150 independent variables in a multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_DATA_POINTS = 150\n",
    "\n",
    "stock_data = stock_dfs['IXIC']\n",
    "stock_data = pick_feature_columns(stock_data)\n",
    "\n",
    "train_data, test_data = train_test_split(stock_data)\n",
    "X_train, y_train = dependent_independent_split(train_data, NUMBER_DATA_POINTS)\n",
    "X_test, y_test = dependent_independent_split(test_data, NUMBER_DATA_POINTS)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "plt.plot(train_data, 'b')\n",
    "plt.plot(test_data, 'r')\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"CloseUSD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "mlr_model = LinearRegression(n_jobs=1)\n",
    "mlr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print(\"RMSE: \", mean_squared_error(y_test, predictions, squared=False))\n",
    "print(\"R-squared: \", r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "plt.plot(predictions, 'r')\n",
    "plt.plot(y_test, 'b')\n",
    "plt.grid(True)\n",
    "ax.set_xlabel(\"Timestep\")\n",
    "ax.set_ylabel(\"CloseUSD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mean = X_test.mean(axis=1)\n",
    "y_pred = mlr_model.predict(X_test)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "plt.scatter(X_test_mean, y_test, color='k')\n",
    "plt.plot(X_test_mean, y_pred, color='r')\n",
    "ax.set_xlabel('CloseUSD')\n",
    "ax.set_ylabel('CloseUSD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Various Scalers\n",
    "In this section, I try and determine if any of the scaling techniques have a noticable impact on the outcome of the model accuracy metrics. I try MinMaxScaler, StandardScaler, and no scaling. The result is that the scaling does not have any affect on the accuracy metrics of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No scaling\n",
    "NUMBER_DATA_POINTS = 1\n",
    "\n",
    "performances = {}\n",
    "for name, df in stock_dfs.items():\n",
    "    df = pick_feature_columns(df)\n",
    "    \n",
    "    train_data, test_data = train_test_split(df)\n",
    "    X_train, y_train = dependent_independent_split(train_data, NUMBER_DATA_POINTS)\n",
    "    X_test, y_test = dependent_independent_split(test_data, NUMBER_DATA_POINTS)\n",
    "    \n",
    "    model = train_model(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    scores = score_model(None, y_test, predictions)\n",
    "    performances[name] = scores\n",
    "\n",
    "performances_df = pd.DataFrame.from_dict(data=performances, orient='index')\n",
    "performances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score scaling\n",
    "NUMBER_DATA_POINTS = 1\n",
    "\n",
    "performances = {}\n",
    "for name, df in stock_dfs.items():\n",
    "    df = pick_feature_columns(df)\n",
    "    scaler, df = standard_scale_data(df)\n",
    "    \n",
    "    train_data, test_data = train_test_split(df)\n",
    "    X_train, y_train = dependent_independent_split(train_data, NUMBER_DATA_POINTS)\n",
    "    X_test, y_test = dependent_independent_split(test_data, NUMBER_DATA_POINTS)\n",
    "    \n",
    "    model = train_model(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    scores = score_model(scaler, y_test, predictions)\n",
    "    performances[name] = scores\n",
    "\n",
    "performances_df = pd.DataFrame.from_dict(data=performances, orient='index')\n",
    "performances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax scaling\n",
    "NUMBER_DATA_POINTS = 1\n",
    "\n",
    "performances = {}\n",
    "for name, df in stock_dfs.items():\n",
    "    df = pick_feature_columns(df)\n",
    "    scaler, df = min_max_scale_data(df)\n",
    "    \n",
    "    train_data, test_data = train_test_split(df)\n",
    "    X_train, y_train = dependent_independent_split(train_data, NUMBER_DATA_POINTS)\n",
    "    X_test, y_test = dependent_independent_split(test_data, NUMBER_DATA_POINTS)\n",
    "    \n",
    "    model = train_model(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    scores = score_model(scaler, y_test, predictions)\n",
    "    performances[name] = scores\n",
    "\n",
    "performances_df = pd.DataFrame.from_dict(data=performances, orient='index')\n",
    "performances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression with Varying Number of Independent Variables\n",
    "To determine how effective varying the amount of independent variables is on the accuracy of the model, I complete a simple/multi-variable linear regression and vary the amount of previous days data. The outcome is that it has little to no impact on the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "number_of_points = [1, 10, 100]\n",
    "\n",
    "for n in number_of_points:\n",
    "    stock_data = stock_dfs['HSI']\n",
    "    stock_data = pick_feature_columns(stock_data)\n",
    "    scaler, stock_data = standard_scale_data(stock_data)\n",
    "\n",
    "    train_data, test_data = train_test_split(stock_data)\n",
    "    X_train, y_train = dependent_independent_split(train_data, n)\n",
    "    X_test, y_test = dependent_independent_split(test_data, n)\n",
    "    \n",
    "    mlr_model = LinearRegression(n_jobs=-1)\n",
    "    mlr_model.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = mlr_model.predict(X_test)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    y_test_scaled = scaler.inverse_transform(y_test)\n",
    "    \n",
    "    print(\"n\", n)\n",
    "    print(\"\\tRMSE: \", mean_squared_error(y_test_scaled, predictions, squared=False))\n",
    "    print(\"\\tR-squared: \", r2_score(y_test_scaled, predictions))\n",
    "    print(\"\\tMAPE\", mean_absolute_percentage_error(y_test_scaled, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "all_scores = {}\n",
    "\n",
    "for name, df in stock_dfs.items(): \n",
    "    scores = []\n",
    "    for n in range(1, 50):\n",
    "        stock_data = df\n",
    "        stock_data = pick_feature_columns(stock_data)\n",
    "        scaler, stock_data = standard_scale_data(stock_data)\n",
    "\n",
    "        train_data, test_data = train_test_split(stock_data)\n",
    "        X_train, y_train = dependent_independent_split(train_data, n)\n",
    "        X_test, y_test = dependent_independent_split(test_data, n)\n",
    "\n",
    "        mlr_model = LinearRegression(n_jobs=-1)\n",
    "        mlr_model.fit(X_train, y_train)\n",
    "\n",
    "        predictions = mlr_model.predict(X_test)\n",
    "        predictions = scaler.inverse_transform(predictions)\n",
    "        y_test_scaled = scaler.inverse_transform(y_test)\n",
    "\n",
    "        scores.append(mean_squared_error(y_test_scaled, predictions, squared=False))\n",
    "        \n",
    "    all_scores[name] = scores\n",
    "\n",
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "for _, score in all_scores.items():\n",
    "    score = minmax_scale(score)\n",
    "    plt.plot(score)\n",
    "\n",
    "ax.set_xlabel(\"Number of independent variables\")\n",
    "ax.set_ylabel(\"RMSE score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression on All Stocks\n",
    "This is to collect the mean of all the accuracy metrics for a mutli-varable linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_DATA_POINTS = 90\n",
    "\n",
    "def process_df(df):\n",
    "    df = pick_feature_columns(df)\n",
    "    train_data, test_data = train_test_split(df)\n",
    "    X_train, y_train = dependent_independent_split(train_data, NUMBER_DATA_POINTS)\n",
    "    X_test, y_test = dependent_independent_split(test_data, NUMBER_DATA_POINTS)\n",
    "    return X_train, X_test, y_train, y_test, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances = {}\n",
    "for name, df in stock_dfs.items():\n",
    "    X_train, X_test, y_train, y_test, scaler = process_df(df)\n",
    "    model = train_model(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    scores = score_model(None, y_test, predictions)\n",
    "    performances[name] = scores\n",
    "\n",
    "performances_df = pd.DataFrame.from_dict(data=performances, orient='index')\n",
    "performances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_df.mean()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
